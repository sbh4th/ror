@article{ardern2023,
  title = {Three Years of Quality Assurance Data Assessing the Performance of over 4000 Grant Peer Review Contributions to the {{Canadian Institutes}} of {{Health Research Project Grant Competition}}},
  author = {Ardern, Clare L. and Martino, Nadia and Nag, Sammy and Tamblyn, Robyn and Moher, David and Mota, Adrian and Khan, Karim M.},
  year = {2023},
  month = jan,
  journal = {FACETS},
  volume = {8},
  pages = {1--14},
  publisher = {Canadian Science Publishing},
  doi = {10.1139/facets-2022-0175},
  urldate = {2025-01-29},
  abstract = {The Canadian Institutes of Health Research (CIHR) commenced a Quality Assurance Program in 2019 to monitor the quality of peer review in its Project Grant Competition Peer Review Committees. Our primary aim was to describe the performance of CIHR grant peer reviewers, based on the assessments made by CIHR peer review leaders during the first 3\,years of the Research Quality Assurance Program. All Peer Review Committee Chairs and (or) Scientific Officers who led peer review for CIHR in 2019, 2020, and 2021 completed Reviewer Quality Feedback forms immediately following Peer Review Committee meetings. The form assessed Performance, Future potential, Review quality, Participation, and Responsiveness. We summarised and descriptively synthesised data from assessments conducted after each of the four grant competitions. The performance of peer reviewers on 4438 occasions was rated by Chairs and Scientific Officers. Approximately one in three peer reviewers submitted outstanding reviews or discussed additional applications and one in 10 demonstrated potential as a future Peer Review Committee leader. At most, one in 20 peer reviewers was considered to have not performed adequately with respect to review quality, participation, or responsiveness. There is a need for more research on the processes involved in allocating research grant funding.}
}

@article{bjerke,
  title = {Funding {{Science}} in an {{Uncertain World}}: {{The Role}} of {{Information}} and {{Incentives}}},
  author = {Bjerke, Christian Hemmestad and Ottaviani, Marco and Oxley, Kristin},
  langid = {english},
  file = {/Users/samharper/Zotero/storage/TUMCX7KJ/Bjerke et al. - Funding Science in an Uncertain World The Role of.pdf}
}

@article{burns2019,
  title = {Gender Differences in Grant and Personnel Award Funding Rates at the {{Canadian Institutes}} of {{Health Research}} Based on Research Content Area: {{A}} Retrospective Analysis},
  shorttitle = {Gender Differences in Grant and Personnel Award Funding Rates at the {{Canadian Institutes}} of {{Health Research}} Based on Research Content Area},
  author = {Burns, Karen E. A. and Straus, Sharon E. and Liu, Kuan and Rizvi, Leena and Guyatt, Gordon},
  editor = {Weiss, Bjoern},
  year = {2019},
  month = oct,
  journal = {PLOS Medicine},
  volume = {16},
  number = {10},
  pages = {e1002935},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.1002935},
  urldate = {2025-02-11},
  langid = {english}
}

@article{erosheva2020,
  title = {{{NIH}} Peer Review: {{Criterion}} Scores Completely Account for Racial Disparities in Overall Impact Scores},
  shorttitle = {{{NIH}} Peer Review},
  author = {Erosheva, Elena A. and Grant, Sheridan and Chen, Mei-Ching and Lindner, Mark D. and Nakamura, Richard K. and Lee, Carole J.},
  year = {2020},
  month = jun,
  journal = {Science Advances},
  volume = {6},
  number = {23},
  pages = {eaaz4868},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/sciadv.aaz4868},
  urldate = {2025-01-28},
  abstract = {Previous research has found that funding disparities are driven by applications' final impact scores and that only a portion of the black/white funding gap can be explained by bibliometrics and topic choice. Using National Institutes of Health R01 applications for council years 2014--2016, we examine assigned reviewers' preliminary overall impact and criterion scores to evaluate whether racial disparities in impact scores can be explained by application and applicant characteristics. We hypothesize that differences in commensuration---the process of combining criterion scores into overall impact scores---disadvantage black applicants. Using multilevel models and matching on key variables including career stage, gender, and area of science, we find little evidence for racial disparities emerging in the process of combining preliminary criterion scores into preliminary overall impact scores. Instead, preliminary criterion scores fully account for racial disparities---yet do not explain all of the variability---in preliminary overall impact scores.}
}

@article{fogelholm2012,
  title = {Panel Discussion Does Not Improve Reliability of Peer Review for Medical Research Grant Proposals},
  author = {Fogelholm, Mikael and Leppinen, Saara and Auvinen, Anssi and Raitanen, Jani and Nuutinen, Anu and V{\"a}{\"a}n{\"a}nen, Kalervo},
  year = {2012},
  month = jan,
  journal = {Journal of Clinical Epidemiology},
  volume = {65},
  number = {1},
  pages = {47--52},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2011.05.001},
  urldate = {2025-02-05},
  abstract = {Objective Peer review is the gold standard for evaluating scientific quality. Compared with studies on inter-reviewer variability, research on panel evaluation is scarce. To appraise the reliability of panel evaluations in grant review, we compared scores by two expert panels reviewing the same grant proposals. Our main interest was to evaluate whether panel discussion improves reliability. Methods Thirty reviewers were randomly allocated to one of the two panels. Sixty-five grant proposals in the fields of clinical medicine and epidemiology were reviewed by both panels. All reviewers received 5--12 proposals. Each proposal was evaluated by two reviewers, using a six-point scale. The reliability of reviewer and panel scores was evaluated using Cohen's kappa with linear weighting. In addition, reliability was also evaluated for the panel mean scores (mean of reviewer scores was used as panel score). Results The proportion of large differences (at least two points) was 40\% for reviewers in panel A, 36\% for reviewers in panel B, 26\% for the panel discussion scores, and 14\% when the means of the two reviewer scores were used. The kappa for panel score after discussion was 0.23 (95\% confidence interval: 0.08, 0.39). By using the mean of the reviewer scores, the panel coefficient was similarly 0.23 (0.00, 0.46). Conclusion The reliability between panel scores was higher than between reviewer scores. The similar interpanel reliability, when using the final panel score or the mean value of reviewer scores, indicates that panel discussions per se did not improve the reliability of the evaluation.},
  keywords = {Consistency,Funding,Inter-reviewer reliability,Interpanel reliability,Peer review,Quality assurance}
}

@article{gregory2024,
  title = {Peer Review in Funding Organizations: {{An}} Analytical Literature Review. ({{RoRI Working Paper No}}.11)},
  shorttitle = {{\textbf{Peer Review in Funding Organizations}}},
  author = {Gregory, Kathleen and Waltman, Ludo and Pinfield, Stephen},
  year = {2024},
  pages = {1085074 Bytes},
  publisher = {Research on Research Institute},
  doi = {10.6084/M9.FIGSHARE.26861680.V5},
  urldate = {2025-01-27},
  abstract = {Peer review performs a crucial and expanding role across the research system. Reviewers evaluate articles, grant proposals, tenure and promotion applications, and research units. Peer review across these \emph{domains} is prone to common problems, such as the risk of bias, a lack of transparency, and system overload (Smith, 2006; Tennant \&amp; Ross-Hellauer, 2020). It also involves common actors, artefacts, and processes, with the same people often doing duplicative reviewing work -- adding burden to an already overburdened system. Despite potential synergies across areas where peer review occurs, current analyses examine particular domains in isolation, e.g., focusing only on peer review in publishing or solely on peer review within funding organizations.How can these systems of peer review work better together? Where do synergies exist? How can peer review be streamlined to reduce burden and increase efficiency? This working paper is part of a broader project to develop a systematic understanding of the goals and characteristics of peer review across domains to develop solutions targeting these questions.Here, we build on our earlier taxonomy of peer review practices in the publishing domain (Kaltenbrunner et al., 2022) to map out the characteristics of peer review within funding organizations. We present the results of an analytical literature review according to this taxonomy; we conclude by considering possible points for synergies across domains where peer review takes place and pose next steps for future research.~{$<$}br{$>$}},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {{Research, science and technology policy}}
}

@article{lee2015,
  title = {Commensuration {{Bias}} in {{Peer Review}}},
  author = {Lee, Carole J.},
  year = {2015},
  month = dec,
  journal = {Philosophy of Science},
  volume = {82},
  number = {5},
  pages = {1272--1283},
  issn = {0031-8248, 1539-767X},
  doi = {10.1086/683652},
  urldate = {2025-01-29},
  abstract = {To arrive at their final evaluation of a manuscript or grant proposal, reviewers must convert a submission's strengths and weaknesses for heterogeneous peer review criteria into a single metric of quality or merit. I identify this process of commensuration as the locus for a new kind of peer review bias. Commensuration bias illuminates how the systematic prioritization of some peer review criteria over others permits and facilitates problematic patterns of publication and funding in science. Commensuration bias also foregrounds a range of structural strategies for realigning peer review practices and institutions with the aims of science.},
  copyright = {https://www.cambridge.org/core/terms},
  langid = {english}
}

@article{materia2015,
  title = {Understanding the Selection Processes of Public Research Projects in Agriculture: {{The}} Role of Scientific Merit},
  shorttitle = {Understanding the Selection Processes of Public Research Projects in Agriculture},
  author = {Materia, V. C. and Pascucci, S. and Kolympiris, C.},
  year = {2015},
  month = oct,
  journal = {Food Policy},
  volume = {56},
  pages = {87--99},
  issn = {0306-9192},
  doi = {10.1016/j.foodpol.2015.08.003},
  urldate = {2025-02-11},
  abstract = {This paper analyses factors that affect the funding of agricultural research projects by regional governments and other regional public authorities. We study the selection process of agricultural research projects funded by the Emilia Romagna regional government in Italy, which follows funding procedures similar to many other European regional public authorities. Leveraging a unique dataset, a Heckman selection model demonstrates that the scientific merit of proposed projects is the primary selection criterion. Still, factors such as the experience of the proposal's reviewers and the gender composition of the reviewing team also influence whether or not a submitted proposal receives funding as well as the amount allocated to the proposal.},
  keywords = {Agriculture,Italy,Peer-review,Research projects,Selection biases}
}

@article{obrecht2007,
  title = {Examining the Value Added by Committee Discussion in the Review of Applications for Research Awards},
  author = {Obrecht, Michael and Tibelius, Karl and D'Aloisio, Guy},
  year = {2007},
  month = jun,
  journal = {Research Evaluation},
  volume = {16},
  number = {2},
  pages = {79--91},
  issn = {0958-2029},
  doi = {10.3152/095820207X223785},
  urldate = {2025-02-12},
  abstract = {We examined a process for evaluating research fellowship proposals in which each was assigned to two members of a review committee for in-depth assessment. Before the committee meeting the reviewers scored the proposal against weighted criteria using benchmarked scales and a detailed rating guide. At the meeting they presented their reviews and then received questions and comments from colleagues. Subsequently each committee member assigned a score reflecting their overall appreciation of the proposal. We observed committees at work, analysed pre-meeting and post-discussion scores and considered feedback from reviewers. Our results suggest that committee discussion and rating of proposals offered no improvement to fairness and effectiveness over and above that attainable from the pre-meeting evaluations.}
}

@article{tamblyn2018,
  title = {Assessment of Potential Bias in Research Grant Peer Review in {{Canada}}},
  author = {Tamblyn, Robyn and Girard, Nadyne and Qian, Christina J. and Hanley, James},
  year = {2018},
  month = apr,
  journal = {Canadian Medical Association Journal},
  volume = {190},
  number = {16},
  pages = {E489-E499},
  issn = {0820-3946, 1488-2329},
  doi = {10.1503/cmaj.170901},
  urldate = {2025-02-12},
  langid = {english}
}

@article{witteman2019,
  title = {Are Gender Gaps Due to Evaluations of the Applicant or the Science? {{A}} Natural Experiment at a National Funding Agency},
  shorttitle = {Are Gender Gaps Due to Evaluations of the Applicant or the Science?},
  author = {Witteman, Holly O and Hendricks, Michael and Straus, Sharon and Tannenbaum, Cara},
  year = {2019},
  month = feb,
  journal = {The Lancet},
  volume = {393},
  number = {10171},
  pages = {531--540},
  issn = {01406736},
  doi = {10.1016/S0140-6736(18)32611-4},
  urldate = {2025-02-11},
  langid = {english}
}
